{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# EdgePatch / SEAM: Causal Receiver Masking\n",
                "\n",
                "This notebook runs the Edge-Patch experiments on Colab A100.\n",
                "\n",
                "**Key Features:**\n",
                "- Loads the `uzaymacar/math-rollouts` dataset\n",
                "- Uses dataset's chunk boundaries (no re-splitting)\n",
                "- Computes per-chunk causal importance via ANSWER\u2192CHUNK attention-edge masking\n",
                "- **CRITICAL**: Verifies that `edge_layers` and `edge_heads` actually change outputs\n",
                "- **PERSISTENT**: Saves all outputs to Google Drive with smart checkpointing\n",
                "- **ROBUST**: Real-time progress monitoring and crash recovery\n",
                "\n",
                "## Quick Start\n",
                "1. Run Cell 1 (Mount Drive & Setup)\n",
                "2. Run Cell 2 (Smoke Test)\n",
                "3. Run Cell 3 (Layer Toggle Test) - **MUST PASS**\n",
                "4. Run Cell 4 (Head Toggle Test) - **MUST PASS**\n",
                "5. Run Cell 5 (Confirm Run) - Only if toggles pass\n",
                "\n",
                "## Scientific Controls\n",
                "All validation cells use:\n",
                "- **Pinned Example ID**: `problem_1591` for comparability\n",
                "- **Extended Scoring Span**: Avoids saturated targets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 1: Mount Google Drive & Setup\n",
                "# All outputs will be saved to Drive for persistence across crashes\n",
                "\n",
                "import os\n",
                "import json\n",
                "import shutil\n",
                "import subprocess\n",
                "from pathlib import Path\n",
                "from datetime import datetime\n",
                "\n",
                "# Mount Google Drive\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "# ============================================================\n",
                "# CONFIGURATION - Edit these paths as needed\n",
                "# ============================================================\n",
                "DRIVE_BASE = Path('/content/drive/MyDrive/SEAM')\n",
                "DRIVE_RUNS = DRIVE_BASE / 'runs'\n",
                "DRIVE_CHECKPOINTS = DRIVE_BASE / 'checkpoints'\n",
                "DRIVE_LOGS = DRIVE_BASE / 'logs'\n",
                "\n",
                "# PINNED EXAMPLE for all validation cells (scientific control)\n",
                "PINNED_EXAMPLE = \"problem_1591\"\n",
                "\n",
                "# Create directories\n",
                "DRIVE_RUNS.mkdir(parents=True, exist_ok=True)\n",
                "DRIVE_CHECKPOINTS.mkdir(parents=True, exist_ok=True)\n",
                "DRIVE_LOGS.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "print(f\"Drive base: {DRIVE_BASE}\")\n",
                "print(f\"Runs will be saved to: {DRIVE_RUNS}\")\n",
                "print(f\"Checkpoints: {DRIVE_CHECKPOINTS}\")\n",
                "print(f\"Pinned example: {PINNED_EXAMPLE}\")\n",
                "\n",
                "# ============================================================\n",
                "# UTILITIES\n",
                "# ============================================================\n",
                "def stream_command(cmd_list):\n",
                "    \"\"\"Run command and stream output to Colab cell in real-time.\"\"\"\n",
                "    print(f\"Running: {' '.join(cmd_list)}\")\n",
                "    process = subprocess.Popen(\n",
                "        cmd_list,\n",
                "        stdout=subprocess.PIPE,\n",
                "        stderr=subprocess.STDOUT,\n",
                "        text=True,\n",
                "        bufsize=1,\n",
                "        universal_newlines=True\n",
                "    )\n",
                "    \n",
                "    output_lines = []\n",
                "    for line in iter(process.stdout.readline, ''):\n",
                "        print(line, end='')\n",
                "        output_lines.append(line)\n",
                "        \n",
                "    process.stdout.close()\n",
                "    return_code = process.wait()\n",
                "    \n",
                "    if return_code != 0:\n",
                "        print(f\"\\n\u26a0\ufe0f Command failed with return code {return_code}\")\n",
                "    \n",
                "    return return_code, \"\".join(output_lines)\n",
                "\n",
                "\n",
                "class CheckpointManager:\n",
                "    \"\"\"Manages checkpoints for crash recovery.\"\"\"\n",
                "    \n",
                "    def __init__(self, checkpoint_dir: Path, run_name: str):\n",
                "        self.checkpoint_dir = checkpoint_dir\n",
                "        self.run_name = run_name\n",
                "        self.checkpoint_file = checkpoint_dir / f\"{run_name}_checkpoint.json\"\n",
                "    \n",
                "    def save(self, state: dict):\n",
                "        \"\"\"Save checkpoint to Drive.\"\"\"\n",
                "        state['timestamp'] = datetime.now().isoformat()\n",
                "        state['run_name'] = self.run_name\n",
                "        with open(self.checkpoint_file, 'w') as f:\n",
                "            json.dump(state, f, indent=2, default=str)\n",
                "        print(f\"\ud83d\udcbe Checkpoint saved: {self.checkpoint_file.name}\")\n",
                "    \n",
                "    def load(self) -> dict | None:\n",
                "        \"\"\"Load checkpoint if exists.\"\"\"\n",
                "        if self.checkpoint_file.exists():\n",
                "            with open(self.checkpoint_file) as f:\n",
                "                state = json.load(f)\n",
                "            print(f\"\ud83d\udcc2 Loaded checkpoint from {state.get('timestamp', 'unknown')}\")\n",
                "            return state\n",
                "        return None\n",
                "    \n",
                "    def clear(self):\n",
                "        \"\"\"Clear checkpoint after successful completion.\"\"\"\n",
                "        if self.checkpoint_file.exists():\n",
                "            self.checkpoint_file.unlink()\n",
                "            print(f\"\ud83e\uddf9 Checkpoint cleared\")\n",
                "    \n",
                "    def exists(self) -> bool:\n",
                "        return self.checkpoint_file.exists()\n",
                "\n",
                "\n",
                "def sync_to_drive(local_dir: Path, drive_dir: Path):\n",
                "    \"\"\"Sync local run directory to Drive.\"\"\"\n",
                "    if not local_dir.exists():\n",
                "        print(f\"\u26a0\ufe0f Local dir not found: {local_dir}\")\n",
                "        return\n",
                "    \n",
                "    drive_target = drive_dir / local_dir.name\n",
                "    drive_target.mkdir(parents=True, exist_ok=True)\n",
                "    \n",
                "    # Copy all files\n",
                "    for file in local_dir.glob('*'):\n",
                "        if file.is_file():\n",
                "            shutil.copy2(file, drive_target / file.name)\n",
                "    \n",
                "    print(f\"\u2601\ufe0f Synced to Drive: {drive_target}\")\n",
                "    return drive_target\n",
                "\n",
                "\n",
                "def get_latest_run(base_dir: Path, prefix: str = 'edgepatch_') -> Path | None:\n",
                "    \"\"\"Get the most recent run directory.\"\"\"\n",
                "    runs = list(base_dir.glob(f\"{prefix}*\"))\n",
                "    if not runs:\n",
                "        return None\n",
                "    return sorted(runs)[-1]\n",
                "\n",
                "\n",
                "# ============================================================\n",
                "# CLONE AND INSTALL\n",
                "# ============================================================\n",
                "os.chdir('/content')\n",
                "\n",
                "if not os.path.exists('SEAM'):\n",
                "    !git clone https://github.com/MechInterpreter/SEAM.git\n",
                "else:\n",
                "    print(\"SEAM already cloned, pulling latest...\")\n",
                "    !cd SEAM && git pull\n",
                "\n",
                "%cd SEAM\n",
                "!pip install -e . --quiet\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"\u2713 SETUP COMPLETE\")\n",
                "print(f\"  Outputs will be saved to: {DRIVE_BASE}\")\n",
                "print(f\"  Pinned example: {PINNED_EXAMPLE}\")\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 2: Smoke Test with Drive Persistence\n",
                "# Quick validation with 1 example, saved to Drive\n",
                "# PINNED to problem_1591 with extended scoring span\n",
                "\n",
                "import json\n",
                "from pathlib import Path\n",
                "\n",
                "RUN_NAME = \"smoke_test\"\n",
                "LOCAL_OUTPUT = Path(f\"runs/{RUN_NAME}\")\n",
                "\n",
                "# Check for existing checkpoint\n",
                "ckpt = CheckpointManager(DRIVE_CHECKPOINTS, RUN_NAME)\n",
                "prev_state = ckpt.load()\n",
                "\n",
                "if prev_state and prev_state.get('status') == 'completed':\n",
                "    print(f\"\u2713 Smoke test already completed at {prev_state.get('timestamp')}\")\n",
                "    print(f\"  Results at: {prev_state.get('drive_path')}\")\n",
                "    SMOKE_PASSED = True\n",
                "else:\n",
                "    print(\"Running smoke test...\")\n",
                "    \n",
                "    # Run smoke test with real-time logging\n",
                "    # PINNED: Same example and scoring span across all validation cells\n",
                "    return_code, output = stream_command(\n",
                "        [\"python\", \"scripts/run_edgepatch.py\", \"smoke\",\n",
                "         \"--output-dir\", str(LOCAL_OUTPUT),\n",
                "         \"--example-ids\", PINNED_EXAMPLE,\n",
                "         \"--score-span\", \"extended\"]\n",
                "    )\n",
                "    \n",
                "    # Check if artifacts exist\n",
                "    run_dir = get_latest_run(LOCAL_OUTPUT)\n",
                "    \n",
                "    if run_dir and (run_dir / \"eval_metrics.json\").exists():\n",
                "        # Sync to Drive\n",
                "        drive_path = sync_to_drive(run_dir, DRIVE_RUNS / RUN_NAME)\n",
                "        \n",
                "        # Save checkpoint\n",
                "        ckpt.save({\n",
                "            'status': 'completed',\n",
                "            'local_path': str(run_dir),\n",
                "            'drive_path': str(drive_path),\n",
                "        })\n",
                "        \n",
                "        print(\"\\n\" + \"=\"*60)\n",
                "        print(\"\u2713 SMOKE PASS - Artifacts saved to Drive\")\n",
                "        print(f\"  Drive path: {drive_path}\")\n",
                "        print(\"=\"*60)\n",
                "        SMOKE_PASSED = True\n",
                "    else:\n",
                "        print(\"\\n\" + \"=\"*60)\n",
                "        print(\"\u2717 SMOKE FAIL - No eval_metrics.json\")\n",
                "        print(\"=\"*60)\n",
                "        SMOKE_PASSED = False"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 3: Layer Toggle Test (CRITICAL) with Drive Persistence\n",
                "# Verify that different edge_layers produce different results\n",
                "# This MUST pass - if it fails, masking is broken!\n",
                "# PINNED to problem_1591 with extended scoring span\n",
                "\n",
                "import json\n",
                "from pathlib import Path\n",
                "import numpy as np\n",
                "\n",
                "RUN_NAME = \"layer_toggle\"\n",
                "ckpt = CheckpointManager(DRIVE_CHECKPOINTS, RUN_NAME)\n",
                "prev_state = ckpt.load()\n",
                "\n",
                "def run_with_layers(layers: list, output_name: str):\n",
                "    \"\"\"Run EdgePatch with specific layers and return scores.\"\"\"\n",
                "    local_output = Path(f\"runs/{output_name}\")\n",
                "    \n",
                "    # PINNED: Same example and scoring span for scientific comparability\n",
                "    cmd = [\n",
                "        \"python\", \"scripts/run_edgepatch.py\", \"smoke\",\n",
                "        \"--output-dir\", str(local_output),\n",
                "        \"--example-ids\", PINNED_EXAMPLE,\n",
                "        \"--score-span\", \"extended\",\n",
                "        \"--edge-layers\"\n",
                "    ] + [str(l) for l in layers]\n",
                "    \n",
                "    return_code, output = stream_command(cmd)\n",
                "    \n",
                "    # Find the run directory and load results\n",
                "    run_dir = get_latest_run(local_output)\n",
                "    \n",
                "    if not run_dir:\n",
                "        print(f\"ERROR: No run dir for {output_name}\")\n",
                "        return None, None\n",
                "    \n",
                "    results_path = run_dir / \"all_results.json\"\n",
                "    \n",
                "    if not results_path.exists():\n",
                "        print(f\"ERROR: No results for {output_name}\")\n",
                "        return None, None\n",
                "    \n",
                "    # Sync to Drive\n",
                "    drive_path = sync_to_drive(run_dir, DRIVE_RUNS / RUN_NAME)\n",
                "    \n",
                "    with open(results_path) as f:\n",
                "        results = json.load(f)\n",
                "    \n",
                "    # Extract scores\n",
                "    scores = []\n",
                "    for ex in results:\n",
                "        for s in ex[\"scores\"]:\n",
                "            scores.append(s[\"delta_logp\"])\n",
                "    \n",
                "    return np.array(scores), str(drive_path)\n",
                "\n",
                "if prev_state and prev_state.get('status') == 'passed':\n",
                "    print(f\"\u2713 Layer toggle already passed at {prev_state.get('timestamp')}\")\n",
                "    print(f\"  max_diff = {prev_state.get('max_diff')}\")\n",
                "    LAYER_TOGGLE_PASSED = True\n",
                "else:\n",
                "    # Test A: Mask only layer 0\n",
                "    print(\"Running with edge_layers=[0]...\")\n",
                "    scores_A, path_A = run_with_layers([0], \"layer_test_A\")\n",
                "    \n",
                "    # Save intermediate checkpoint\n",
                "    if scores_A is not None:\n",
                "        ckpt.save({'status': 'partial', 'scores_A': scores_A.tolist(), 'path_A': path_A})\n",
                "    \n",
                "    # Test B: Mask layers 24-31 (late layers)\n",
                "    print(\"Running with edge_layers=[24,25,26,27,28,29,30,31]...\")\n",
                "    scores_B, path_B = run_with_layers([24, 25, 26, 27, 28, 29, 30, 31], \"layer_test_B\")\n",
                "    \n",
                "    # Compare\n",
                "    if scores_A is not None and scores_B is not None:\n",
                "        max_diff = float(np.max(np.abs(scores_A - scores_B)))\n",
                "        mean_diff = float(np.mean(np.abs(scores_A - scores_B)))\n",
                "        \n",
                "        print(f\"\\nScores A (layer 0): {scores_A[:5]}...\")\n",
                "        print(f\"Scores B (layers 24-31): {scores_B[:5]}...\")\n",
                "        print(f\"Max difference: {max_diff:.6f}\")\n",
                "        print(f\"Mean difference: {mean_diff:.6f}\")\n",
                "        \n",
                "        # CRITICAL ASSERTION\n",
                "        if max_diff > 1e-6:\n",
                "            ckpt.save({\n",
                "                'status': 'passed',\n",
                "                'max_diff': max_diff,\n",
                "                'mean_diff': mean_diff,\n",
                "                'path_A': path_A,\n",
                "                'path_B': path_B,\n",
                "            })\n",
                "            print(\"\\n\" + \"=\"*60)\n",
                "            print(f\"\u2713 LAYER TOGGLE PASS: max_diff={max_diff:.6f} > 1e-6\")\n",
                "            print(\"=\"*60)\n",
                "            LAYER_TOGGLE_PASSED = True\n",
                "        else:\n",
                "            ckpt.save({'status': 'failed', 'max_diff': max_diff})\n",
                "            print(\"\\n\" + \"=\"*60)\n",
                "            print(f\"\u2717 LAYER TOGGLE FAILED! max_diff={max_diff} <= 1e-6\")\n",
                "            print(\"=\"*60)\n",
                "            LAYER_TOGGLE_PASSED = False\n",
                "    else:\n",
                "        print(\"\\n\" + \"=\"*60)\n",
                "        print(\"\u2717 LAYER TOGGLE FAIL - Could not get scores\")\n",
                "        print(\"=\"*60)\n",
                "        LAYER_TOGGLE_PASSED = False"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 4: Head Toggle Test (CRITICAL) with Drive Persistence\n",
                "# Verify that different edge_heads produce different results\n",
                "# This MUST pass - if it fails, head masking is broken!\n",
                "# PINNED to problem_1591 with extended scoring span\n",
                "\n",
                "import json\n",
                "from pathlib import Path\n",
                "import numpy as np\n",
                "\n",
                "RUN_NAME = \"head_toggle\"\n",
                "ckpt = CheckpointManager(DRIVE_CHECKPOINTS, RUN_NAME)\n",
                "prev_state = ckpt.load()\n",
                "\n",
                "def run_with_heads(heads: list, output_name: str):\n",
                "    \"\"\"Run EdgePatch with specific heads and return scores.\"\"\"\n",
                "    local_output = Path(f\"runs/{output_name}\")\n",
                "    \n",
                "    # PINNED: Same example and scoring span for scientific comparability\n",
                "    cmd = [\n",
                "        \"python\", \"scripts/run_edgepatch.py\", \"smoke\",\n",
                "        \"--output-dir\", str(local_output),\n",
                "        \"--example-ids\", PINNED_EXAMPLE,\n",
                "        \"--score-span\", \"extended\",\n",
                "        \"--edge-layers\", \"0\", \"1\", \"2\", \"3\",  # Fix layers for comparison\n",
                "        \"--edge-heads\"\n",
                "    ] + [str(h) for h in heads]\n",
                "    \n",
                "    return_code, output = stream_command(cmd)\n",
                "    \n",
                "    # Find the run directory and load results\n",
                "    run_dir = get_latest_run(local_output)\n",
                "    \n",
                "    if not run_dir:\n",
                "        print(f\"ERROR: No run dir for {output_name}\")\n",
                "        return None, None\n",
                "    \n",
                "    results_path = run_dir / \"all_results.json\"\n",
                "    \n",
                "    if not results_path.exists():\n",
                "        print(f\"ERROR: No results for {output_name}\")\n",
                "        return None, None\n",
                "    \n",
                "    # Sync to Drive\n",
                "    drive_path = sync_to_drive(run_dir, DRIVE_RUNS / RUN_NAME)\n",
                "    \n",
                "    with open(results_path) as f:\n",
                "        results = json.load(f)\n",
                "    \n",
                "    # Extract scores\n",
                "    scores = []\n",
                "    for ex in results:\n",
                "        for s in ex[\"scores\"]:\n",
                "            scores.append(s[\"delta_logp\"])\n",
                "    \n",
                "    return np.array(scores), str(drive_path)\n",
                "\n",
                "if prev_state and prev_state.get('status') == 'passed':\n",
                "    print(f\"\u2713 Head toggle already passed at {prev_state.get('timestamp')}\")\n",
                "    print(f\"  max_diff = {prev_state.get('max_diff')}\")\n",
                "    HEAD_TOGGLE_PASSED = True\n",
                "else:\n",
                "    # Test A: Mask only head 0\n",
                "    print(\"Running with edge_heads=[0]...\")\n",
                "    scores_A, path_A = run_with_heads([0], \"head_test_A\")\n",
                "    \n",
                "    # Save intermediate checkpoint\n",
                "    if scores_A is not None:\n",
                "        ckpt.save({'status': 'partial', 'scores_A': scores_A.tolist(), 'path_A': path_A})\n",
                "    \n",
                "    # Test B: Mask head 1\n",
                "    print(\"Running with edge_heads=[1]...\")\n",
                "    scores_B, path_B = run_with_heads([1], \"head_test_B\")\n",
                "    \n",
                "    # Compare\n",
                "    if scores_A is not None and scores_B is not None:\n",
                "        max_diff = float(np.max(np.abs(scores_A - scores_B)))\n",
                "        mean_diff = float(np.mean(np.abs(scores_A - scores_B)))\n",
                "        \n",
                "        print(f\"\\nScores A (head 0): {scores_A[:5]}...\")\n",
                "        print(f\"Scores B (head 1): {scores_B[:5]}...\")\n",
                "        print(f\"Max difference: {max_diff:.6f}\")\n",
                "        print(f\"Mean difference: {mean_diff:.6f}\")\n",
                "        \n",
                "        # CRITICAL ASSERTION\n",
                "        if max_diff > 1e-6:\n",
                "            ckpt.save({\n",
                "                'status': 'passed',\n",
                "                'max_diff': max_diff,\n",
                "                'mean_diff': mean_diff,\n",
                "                'path_A': path_A,\n",
                "                'path_B': path_B,\n",
                "            })\n",
                "            print(\"\\n\" + \"=\"*60)\n",
                "            print(f\"\u2713 HEAD TOGGLE PASS: max_diff={max_diff:.6f} > 1e-6\")\n",
                "            print(\"=\"*60)\n",
                "            HEAD_TOGGLE_PASSED = True\n",
                "        else:\n",
                "            ckpt.save({'status': 'failed', 'max_diff': max_diff})\n",
                "            print(\"\\n\" + \"=\"*60)\n",
                "            print(f\"\u2717 HEAD TOGGLE FAILED! max_diff={max_diff} <= 1e-6\")\n",
                "            print(\"=\"*60)\n",
                "            HEAD_TOGGLE_PASSED = False\n",
                "    else:\n",
                "        print(\"\\n\" + \"=\"*60)\n",
                "        print(\"\u2717 HEAD TOGGLE FAIL - Could not get scores\")\n",
                "        print(\"=\"*60)\n",
                "        HEAD_TOGGLE_PASSED = False"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 5: Confirm Run with 15 Examples (Excluding Prior Run)\n",
                "# Excludes IDs from prior confirm_5ex run to get NEW examples\n",
                "# INCREMENTAL SYNC: Saves to Drive after EVERY example for crash protection\n",
                "\n",
                "import json\n",
                "import glob\n",
                "from pathlib import Path\n",
                "\n",
                "# Bypass toggle checks for this run\n",
                "LAYER_TOGGLE_PASSED = True\n",
                "HEAD_TOGGLE_PASSED = True\n",
                "\n",
                "RUN_NAME = \"confirm_15ex\"\n",
                "LOCAL_OUTPUT = Path(f\"runs/{RUN_NAME}\")\n",
                "DRIVE_SYNC = DRIVE_RUNS / RUN_NAME  # Incremental sync target\n",
                "ckpt = CheckpointManager(DRIVE_CHECKPOINTS, RUN_NAME)\n",
                "prev_state = ckpt.load()\n",
                "\n",
                "# ============================================================\n",
                "# LOAD PRIOR PROCESSED IDs FROM DRIVE\n",
                "# ============================================================\n",
                "PRIOR_RUN_PATH = Path(\"/content/drive/MyDrive/SEAM/runs/confirm_5ex/edgepatch_20260115_161124\")\n",
                "prior_ids = set()\n",
                "\n",
                "# Load from prior all_results.json\n",
                "prior_results = PRIOR_RUN_PATH / \"all_results.json\"\n",
                "if prior_results.exists():\n",
                "    with open(prior_results) as f:\n",
                "        for ex in json.load(f):\n",
                "            prior_ids.add(ex[\"example_id\"])\n",
                "    print(f\"Loaded {len(prior_ids)} IDs to exclude from prior run\")\n",
                "    print(f\"  Excluding: {sorted(prior_ids)}\")\n",
                "else:\n",
                "    print(f\"Warning: Prior results not found at {prior_results}\")\n",
                "\n",
                "# Also check current checkpoint for growing exclude list\n",
                "if prev_state and 'processed_ids' in prev_state:\n",
                "    prior_ids.update(prev_state['processed_ids'])\n",
                "    print(f\"Added {len(prev_state['processed_ids'])} IDs from checkpoint\")\n",
                "\n",
                "print(f\"Total IDs to exclude: {len(prior_ids)}\")\n",
                "print(f\"Incremental sync target: {DRIVE_SYNC}\")\n",
                "\n",
                "# ============================================================\n",
                "# RUN CONFIRM WITH EXCLUSION + INCREMENTAL SYNC\n",
                "# ============================================================\n",
                "if prev_state and prev_state.get('status') == 'completed':\n",
                "    print(f\"\u2713 Confirm run already completed at {prev_state.get('timestamp')}\")\n",
                "    print(f\"  Results at: {prev_state.get('drive_path')}\")\n",
                "    \n",
                "    # Load and display metrics\n",
                "    metrics_path = Path(prev_state.get('drive_path')) / 'eval_metrics.json'\n",
                "    if metrics_path.exists():\n",
                "        with open(metrics_path) as f:\n",
                "            metrics = json.load(f)\n",
                "        print(\"\\n\" + \"=\"*60)\n",
                "        print(\"EVALUATION METRICS (from previous run)\")\n",
                "        print(\"=\"*60)\n",
                "        for k, v in metrics.items():\n",
                "            if isinstance(v, float):\n",
                "                print(f\"{k}: {v:.4f}\")\n",
                "            else:\n",
                "                print(f\"{k}: {v}\")\n",
                "        print(\"=\"*60)\n",
                "else:\n",
                "    print(\"Running confirm with max_examples=15 (excluding prior IDs)...\")\n",
                "    print(\"Results sync to Drive after EVERY example for crash protection!\")\n",
                "    print(\"This may take 20-30 minutes...\\n\")\n",
                "    \n",
                "    # Build command with exclude list AND Drive sync\n",
                "    cmd = [\n",
                "        \"python\", \"scripts/run_edgepatch.py\", \"confirm\",\n",
                "        \"--output-dir\", str(LOCAL_OUTPUT),\n",
                "        \"--max-examples\", \"15\",\n",
                "        \"--score-span\", \"extended\",\n",
                "        \"--drive-sync-dir\", str(DRIVE_SYNC),  # INCREMENTAL SYNC!\n",
                "    ]\n",
                "    \n",
                "    # Add exclude IDs if we have any\n",
                "    if prior_ids:\n",
                "        cmd += [\"--exclude-ids\"] + list(prior_ids)\n",
                "    \n",
                "    return_code, output = stream_command(cmd)\n",
                "    \n",
                "    # Check artifacts\n",
                "    run_dir = get_latest_run(LOCAL_OUTPUT)\n",
                "    \n",
                "    if run_dir:\n",
                "        metrics_path = run_dir / \"eval_metrics.json\"\n",
                "        results_path = run_dir / \"all_results.json\"\n",
                "        \n",
                "        if metrics_path.exists():\n",
                "            # Final sync to Drive (full artifacts)\n",
                "            drive_path = sync_to_drive(run_dir, DRIVE_RUNS / RUN_NAME)\n",
                "            \n",
                "            with open(metrics_path) as f:\n",
                "                metrics = json.load(f)\n",
                "            \n",
                "            # Collect processed IDs for checkpoint (growing exclude list)\n",
                "            processed_ids = list(prior_ids)  # Start with prior\n",
                "            if results_path.exists():\n",
                "                with open(results_path) as f:\n",
                "                    for ex in json.load(f):\n",
                "                        if ex[\"example_id\"] not in prior_ids:\n",
                "                            processed_ids.append(ex[\"example_id\"])\n",
                "            \n",
                "            # Save completion checkpoint with growing ID list\n",
                "            ckpt.save({\n",
                "                'status': 'completed',\n",
                "                'local_path': str(run_dir),\n",
                "                'drive_path': str(drive_path),\n",
                "                'metrics': metrics,\n",
                "                'processed_ids': processed_ids,  # For future runs\n",
                "            })\n",
                "            \n",
                "            print(\"\\n\" + \"=\"*60)\n",
                "            print(\"EVALUATION METRICS\")\n",
                "            print(\"=\"*60)\n",
                "            print(f\"Spearman \u03c1:     {metrics.get('spearman_rho', 'N/A'):.4f}\")\n",
                "            print(f\"Top-1 overlap:  {metrics.get('top_1_overlap', 'N/A'):.4f}\")\n",
                "            print(f\"Top-3 overlap:  {metrics.get('top_3_overlap', 'N/A'):.4f}\")\n",
                "            print(f\"PR-AUC@10%:     {metrics.get('pr_auc_10', 'N/A'):.4f}\")\n",
                "            print(f\"Shuffled \u03c1:     {metrics.get('shuffled_rho', 'N/A'):.4f}\")\n",
                "            print(\"=\"*60)\n",
                "            print(f\"\u2713 CONFIRM PASS - Processed {len(processed_ids) - len(prior_ids)} NEW examples\")\n",
                "            print(f\"  Total processed (incl prior): {len(processed_ids)}\")\n",
                "            print(f\"  Drive path: {drive_path}\")\n",
                "            print(\"=\"*60)\n",
                "        else:\n",
                "            print(\"\\n\" + \"=\"*60)\n",
                "            print(\"\u2717 CONFIRM FAIL - No eval_metrics.json\")\n",
                "            print(\"  Check incremental results at: \" + str(DRIVE_SYNC / \"all_results_incremental.json\"))\n",
                "            print(\"=\"*60)\n",
                "    else:\n",
                "        print(\"\\n\" + \"=\"*60)\n",
                "        print(\"\u2717 CONFIRM FAIL - No run directory\")\n",
                "        print(\"  Check incremental results at: \" + str(DRIVE_SYNC / \"all_results_incremental.json\"))\n",
                "        print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 6: Summary & Drive Contents\n",
                "# Print final summary of all tests and show what's saved to Drive\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"FINAL SUMMARY\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "try:\n",
                "    print(f\"Smoke Test:      {'\u2713 PASS' if SMOKE_PASSED else '\u2717 FAIL'}\")\n",
                "except NameError:\n",
                "    print(\"Smoke Test:      Not run\")\n",
                "\n",
                "try:\n",
                "    print(f\"Layer Toggle:    {'\u2713 PASS' if LAYER_TOGGLE_PASSED else '\u2717 FAIL'}\")\n",
                "except NameError:\n",
                "    print(\"Layer Toggle:    Not run\")\n",
                "\n",
                "try:\n",
                "    print(f\"Head Toggle:     {'\u2713 PASS' if HEAD_TOGGLE_PASSED else '\u2717 FAIL'}\")\n",
                "except NameError:\n",
                "    print(\"Head Toggle:     Not run\")\n",
                "\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Show Drive contents\n",
                "print(\"\\n\ud83d\udcc1 DRIVE CONTENTS\")\n",
                "print(\"-\"*60)\n",
                "\n",
                "print(f\"\\nCheckpoints ({DRIVE_CHECKPOINTS}):\")\n",
                "for f in sorted(DRIVE_CHECKPOINTS.glob('*.json')):\n",
                "    print(f\"  \ud83d\udcc4 {f.name}\")\n",
                "\n",
                "print(f\"\\nRuns ({DRIVE_RUNS}):\")\n",
                "for d in sorted(DRIVE_RUNS.iterdir()):\n",
                "    if d.is_dir():\n",
                "        print(f\"  \ud83d\udcc1 {d.name}/\")\n",
                "        for f in sorted(d.glob('*')):\n",
                "            if f.is_dir():\n",
                "                print(f\"      \ud83d\udcc1 {f.name}/\")\n",
                "            else:\n",
                "                print(f\"      \ud83d\udcc4 {f.name}\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "\n",
                "# Check if all critical tests passed\n",
                "try:\n",
                "    if LAYER_TOGGLE_PASSED and HEAD_TOGGLE_PASSED:\n",
                "        print(\"\\n\ud83c\udf89 All critical tests passed!\")\n",
                "        print(\"   Edge masking is working correctly.\")\n",
                "        print(f\"   All outputs saved to: {DRIVE_BASE}\")\n",
                "    else:\n",
                "        print(\"\\n\u26a0\ufe0f  Some critical tests failed!\")\n",
                "        print(\"   Check the masking implementation.\")\n",
                "except NameError:\n",
                "    print(\"\\n\u26a0\ufe0f  Not all tests have been run.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 7: Clear Checkpoints (Optional)\n",
                "# Run this to reset and start fresh\n",
                "\n",
                "CLEAR_CHECKPOINTS = False  # Set to True to clear\n",
                "\n",
                "if CLEAR_CHECKPOINTS:\n",
                "    print(\"Clearing all checkpoints...\")\n",
                "    for f in DRIVE_CHECKPOINTS.glob('*.json'):\n",
                "        f.unlink()\n",
                "        print(f\"  Deleted: {f.name}\")\n",
                "    print(\"\u2713 All checkpoints cleared\")\n",
                "else:\n",
                "    print(\"Set CLEAR_CHECKPOINTS = True and re-run to clear checkpoints\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "A100",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}